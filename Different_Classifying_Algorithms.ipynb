{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Different Classifying Algorithms.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhvaniVora25/Spambase-UCI-Classifier/blob/master/Different_Classifying_Algorithms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "flHVzsAov4MR",
        "colab_type": "code",
        "outputId": "159cc18a-cb4a-4522-e2bf-f079dd02c54f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jxyde_4ZwAXg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/gdrive/My Drive/Colab Notebooks')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ip0neFtOs6J-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "df = pd.read_csv('spambase.data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L_BlXKy4v2EW",
        "colab_type": "code",
        "outputId": "2fd615f8-1ca5-42d5-d2a8-18a0e571ee2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Percentage of spams in the data: \")\n",
        "df['class'].value_counts()*100/df['class'].count()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of spams in the data: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    60.595523\n",
              "1    39.404477\n",
              "Name: class, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "hUYACdniEG8e",
        "colab_type": "code",
        "outputId": "61190fb1-d5a2-40b9-fdef-4335efbb0495",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_freq_make</th>\n",
              "      <th>word_freq_address</th>\n",
              "      <th>word_freq_all</th>\n",
              "      <th>word_freq_3d</th>\n",
              "      <th>word_freq_our</th>\n",
              "      <th>word_freq_over</th>\n",
              "      <th>word_freq_remove</th>\n",
              "      <th>word_freq_internet</th>\n",
              "      <th>word_freq_order</th>\n",
              "      <th>word_freq_mail</th>\n",
              "      <th>...</th>\n",
              "      <th>char_freq_;</th>\n",
              "      <th>char_freq_(</th>\n",
              "      <th>char_freq_[</th>\n",
              "      <th>char_freq_!</th>\n",
              "      <th>char_freq_$</th>\n",
              "      <th>char_freq_#</th>\n",
              "      <th>capital_run_length_average</th>\n",
              "      <th>capital_run_length_longest</th>\n",
              "      <th>capital_run_length_total</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.104553</td>\n",
              "      <td>0.213015</td>\n",
              "      <td>0.280656</td>\n",
              "      <td>0.065425</td>\n",
              "      <td>0.312223</td>\n",
              "      <td>0.095901</td>\n",
              "      <td>0.114208</td>\n",
              "      <td>0.105295</td>\n",
              "      <td>0.090067</td>\n",
              "      <td>0.239413</td>\n",
              "      <td>...</td>\n",
              "      <td>0.038575</td>\n",
              "      <td>0.139030</td>\n",
              "      <td>0.016976</td>\n",
              "      <td>0.269071</td>\n",
              "      <td>0.075811</td>\n",
              "      <td>0.044238</td>\n",
              "      <td>5.191515</td>\n",
              "      <td>52.172789</td>\n",
              "      <td>283.289285</td>\n",
              "      <td>0.394045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.305358</td>\n",
              "      <td>1.290575</td>\n",
              "      <td>0.504143</td>\n",
              "      <td>1.395151</td>\n",
              "      <td>0.672513</td>\n",
              "      <td>0.273824</td>\n",
              "      <td>0.391441</td>\n",
              "      <td>0.401071</td>\n",
              "      <td>0.278616</td>\n",
              "      <td>0.644755</td>\n",
              "      <td>...</td>\n",
              "      <td>0.243471</td>\n",
              "      <td>0.270355</td>\n",
              "      <td>0.109394</td>\n",
              "      <td>0.815672</td>\n",
              "      <td>0.245882</td>\n",
              "      <td>0.429342</td>\n",
              "      <td>31.729449</td>\n",
              "      <td>194.891310</td>\n",
              "      <td>606.347851</td>\n",
              "      <td>0.488698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.588000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.065000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.276000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>95.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.420000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.380000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.188000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.315000</td>\n",
              "      <td>0.052000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.706000</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>266.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>4.540000</td>\n",
              "      <td>14.280000</td>\n",
              "      <td>5.100000</td>\n",
              "      <td>42.810000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>5.880000</td>\n",
              "      <td>7.270000</td>\n",
              "      <td>11.110000</td>\n",
              "      <td>5.260000</td>\n",
              "      <td>18.180000</td>\n",
              "      <td>...</td>\n",
              "      <td>4.385000</td>\n",
              "      <td>9.752000</td>\n",
              "      <td>4.081000</td>\n",
              "      <td>32.478000</td>\n",
              "      <td>6.003000</td>\n",
              "      <td>19.829000</td>\n",
              "      <td>1102.500000</td>\n",
              "      <td>9989.000000</td>\n",
              "      <td>15841.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 58 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
              "count     4601.000000        4601.000000    4601.000000   4601.000000   \n",
              "mean         0.104553           0.213015       0.280656      0.065425   \n",
              "std          0.305358           1.290575       0.504143      1.395151   \n",
              "min          0.000000           0.000000       0.000000      0.000000   \n",
              "25%          0.000000           0.000000       0.000000      0.000000   \n",
              "50%          0.000000           0.000000       0.000000      0.000000   \n",
              "75%          0.000000           0.000000       0.420000      0.000000   \n",
              "max          4.540000          14.280000       5.100000     42.810000   \n",
              "\n",
              "       word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
              "count    4601.000000     4601.000000       4601.000000         4601.000000   \n",
              "mean        0.312223        0.095901          0.114208            0.105295   \n",
              "std         0.672513        0.273824          0.391441            0.401071   \n",
              "min         0.000000        0.000000          0.000000            0.000000   \n",
              "25%         0.000000        0.000000          0.000000            0.000000   \n",
              "50%         0.000000        0.000000          0.000000            0.000000   \n",
              "75%         0.380000        0.000000          0.000000            0.000000   \n",
              "max        10.000000        5.880000          7.270000           11.110000   \n",
              "\n",
              "       word_freq_order  word_freq_mail     ...       char_freq_;  char_freq_(  \\\n",
              "count      4601.000000     4601.000000     ...       4601.000000  4601.000000   \n",
              "mean          0.090067        0.239413     ...          0.038575     0.139030   \n",
              "std           0.278616        0.644755     ...          0.243471     0.270355   \n",
              "min           0.000000        0.000000     ...          0.000000     0.000000   \n",
              "25%           0.000000        0.000000     ...          0.000000     0.000000   \n",
              "50%           0.000000        0.000000     ...          0.000000     0.065000   \n",
              "75%           0.000000        0.160000     ...          0.000000     0.188000   \n",
              "max           5.260000       18.180000     ...          4.385000     9.752000   \n",
              "\n",
              "       char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
              "count  4601.000000  4601.000000  4601.000000  4601.000000   \n",
              "mean      0.016976     0.269071     0.075811     0.044238   \n",
              "std       0.109394     0.815672     0.245882     0.429342   \n",
              "min       0.000000     0.000000     0.000000     0.000000   \n",
              "25%       0.000000     0.000000     0.000000     0.000000   \n",
              "50%       0.000000     0.000000     0.000000     0.000000   \n",
              "75%       0.000000     0.315000     0.052000     0.000000   \n",
              "max       4.081000    32.478000     6.003000    19.829000   \n",
              "\n",
              "       capital_run_length_average  capital_run_length_longest  \\\n",
              "count                 4601.000000                 4601.000000   \n",
              "mean                     5.191515                   52.172789   \n",
              "std                     31.729449                  194.891310   \n",
              "min                      1.000000                    1.000000   \n",
              "25%                      1.588000                    6.000000   \n",
              "50%                      2.276000                   15.000000   \n",
              "75%                      3.706000                   43.000000   \n",
              "max                   1102.500000                 9989.000000   \n",
              "\n",
              "       capital_run_length_total        class  \n",
              "count               4601.000000  4601.000000  \n",
              "mean                 283.289285     0.394045  \n",
              "std                  606.347851     0.488698  \n",
              "min                    1.000000     0.000000  \n",
              "25%                   35.000000     0.000000  \n",
              "50%                   95.000000     0.000000  \n",
              "75%                  266.000000     1.000000  \n",
              "max                15841.000000     1.000000  \n",
              "\n",
              "[8 rows x 58 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "9XGZB9IT9l3J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Without any data standardizations"
      ]
    },
    {
      "metadata": {
        "id": "CdIjVFcFwFRV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y = df[\"class\"]\n",
        "X = df.drop('class', axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EfIfp1lXJDDe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X.capital_run_length_longest.hist(bin=25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ciEJsC-4JOz3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_X, test_X, train_y, test_y = train_test_split(X.as_matrix(), y.as_matrix(), test_size=0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "UXTowZpUAUI1"
      },
      "cell_type": "markdown",
      "source": [
        "Using non linear support vector machine without normalization"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "144a98a2-ab79-4c68-a426-c49fb64d8a83",
        "id": "vSUfvnz0AUI5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "import numpy as np\n",
        "\n",
        "model = svm.SVC(degree=3)\n",
        "model.fit(train_X,train_y)\n",
        "predict_y = model.predict(test_X)\n",
        "predict_y = [1 if y>=0.5 else 0 for y in predict_y]\n",
        "accuracy = np.sum(predict_y == test_y)/len(test_y)\n",
        "print(accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8218940052128584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zUuyd_OdAUJH"
      },
      "cell_type": "markdown",
      "source": [
        "#Using Normalized Data with SVM"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "5518fb15-097a-4b03-e17c-140d51b63294",
        "id": "Ul3QcQZIAUJJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "normalized_X=(X-X.min())/(X.max()-X.min())\n",
        "n_train_X, n_test_X, n_train_y, n_test_y = train_test_split(normalized_X.as_matrix(), y.as_matrix(), test_size=0.25)\n",
        "\n",
        "model = svm.SVC(degree=3)\n",
        "model.fit(n_train_X,n_train_y)\n",
        "predict_y = model.predict(n_test_X)\n",
        "accuracy = np.sum(predict_y == n_test_y)/len(n_test_y)\n",
        "print(accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8071242397914856\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "W8tZdtkSAUJQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "scores = cross_val_score(estimator=model,X=normalized_X, y=y, cv = 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "8c5dce5e-6078-474b-8e7b-c00a4b56991f",
        "id": "kZoVh1GqAUJS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(np.max(scores))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8543478260869565\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "56da7116-2bb0-44f7-f0ea-545cfe92c2ef",
        "id": "DMWtkZZjAUJU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "np.random.seed(5)\n",
        "\n",
        "kmeans = KMeans(n_clusters = 2, random_state = 1).fit(train_X)\n",
        "\n",
        "predict_y = kmeans.predict(test_X)\n",
        "accuracy = np.sum(predict_y == (test_y))/len(test_y)\n",
        "print(accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6264118158123371\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "PId5Qp0kAUJX"
      },
      "cell_type": "markdown",
      "source": [
        "#Decision Tree without and with normalization"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "1d37753a-e85d-4c4a-f93a-5ba42b535a99",
        "id": "hV4wEOmrAUJX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "dtr = DecisionTreeRegressor(random_state=1, splitter='best')\n",
        "dtr.fit(train_X, train_y)\n",
        "predict_y = dtr.predict(test_X)\n",
        "accuracy = np.sum(predict_y == (test_y))/len(test_y)\n",
        "print(accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9252823631624674\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "9c618e6f-9a5f-4992-8b82-9a88fa1ff4c3",
        "id": "PCeU3vteAUJZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "normalized_X=(X-X.min())/(X.max()-X.min())\n",
        "n_train_X, n_test_X, n_train_y, n_test_y = train_test_split(normalized_X.as_matrix(), y.as_matrix(), test_size=0.25)\n",
        "dtr.fit(n_train_X, n_train_y)\n",
        "predict_y = dtr.predict(n_test_X)\n",
        "accuracy = np.sum(predict_y == (n_test_y))/len(n_test_y)\n",
        "print(accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9261511728931364\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "tuHgdNiDAUJc"
      },
      "cell_type": "markdown",
      "source": [
        "#Decision Tree Regressor with standardization (normal distribution of data)"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "96cb9854-f197-435c-f4b9-c1d6ff97ade9",
        "id": "XKIdhvWVAUJd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "standardized_X=(X-X.mean())/X.std()\n",
        "n_train_X, n_test_X, n_train_y, n_test_y = train_test_split(normalized_X.as_matrix(), y.as_matrix(), test_size=0.25)\n",
        "dtr.fit(n_train_X, n_train_y)\n",
        "predict_y = dtr.predict(n_test_X)\n",
        "accuracy = np.sum(predict_y == (n_test_y))/len(n_test_y)\n",
        "print(accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.89748045178106\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "A6Df1StLAUJh"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Logistic Regression without and with normalization with cross validation"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "aa4675ed-13e4-4654-9074-c48d484a8985",
        "id": "WQ3N0gJoAUJi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "clf = LogisticRegressionCV(cv=5, random_state=1,multi_class='ovr', solver='lbfgs').fit(X, y)\n",
        "print(clf.score(X,y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9311019343620952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "2b577d15-c01b-4357-d1d6-706ed1617d7c",
        "id": "a4v1FQLpAUJm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "normalized_X=(X-X.min())/(X.max()-X.min())\n",
        "\n",
        "clf = LogisticRegressionCV(cv=5, random_state=1,multi_class='ovr', solver='lbfgs').fit(normalized_X, y)\n",
        "print(clf.score(normalized_X,y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9330580308628559\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "yL_6aXwBAUJp"
      },
      "cell_type": "markdown",
      "source": [
        "Logistic Regression seems to work the best along with cross validation and normalization.\n",
        "\n",
        "In the following one, I am trying it with some of the specific features and normalization on those features."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "27329767-82cf-4bf5-8806-eb2a9da86162",
        "id": "jRtw50d1AUJq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "df_features = ['capital_run_length_average',\n",
        "               'capital_run_length_longest',\n",
        "               'capital_run_length_total', \n",
        "               'word_freq_credit',\n",
        "               'char_freq_$',\n",
        "               'word_freq_meeting',\n",
        "               'char_freq_!',\n",
        "               'word_freq_conference',\n",
        "               'word_freq_table',\n",
        "                'word_freq_project',\n",
        "               'word_freq_free',\n",
        "               'word_freq_edu',\n",
        "               'word_freq_cs',\n",
        "               'char_freq_;',\n",
        "               'word_freq_direct',\n",
        "               'word_freq_receive',\n",
        "               'word_freq_money',\n",
        "               'word_freq_report',\n",
        "               'word_freq_lab',\n",
        "               'word_freq_pm',\n",
        "               'word_freq_order',\n",
        "               'word_freq_internet',\n",
        "               'word_freq_re',\n",
        "               'word_freq_our',\n",
        "               'char_freq_(',\n",
        "               'char_freq_[',\n",
        "               'word_freq_technology',\n",
        "               'word_freq_parts',\n",
        "               'word_freq_you',\n",
        "               'word_freq_your',\n",
        "               'word_freq_telnet']\n",
        "s_X = df[df_features]\n",
        "normalized_X=(s_X-s_X.min())/(s_X.max()-s_X.min())\n",
        "\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "clf = LogisticRegressionCV(cv=5, random_state=1,multi_class='ovr', solver='lbfgs').fit(normalized_X, y)\n",
        "print(clf.score(normalized_X,y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8969789176266029\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4CKg_aQLAUJs"
      },
      "cell_type": "markdown",
      "source": [
        "AdaBoost with Normalization and Cross Validation"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "743da0c6-6b4a-4f4c-d3ba-7b8c7aff1c6d",
        "id": "fIDN8bslAUJt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "normalized_X=(X-X.mean())/X.max()-X.mean()\n",
        "\n",
        "bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n",
        "                         algorithm=\"SAMME.R\",\n",
        "                         n_estimators=200)\n",
        "\n",
        "\n",
        "\n",
        "scores = cross_val_score(estimator=bdt,X=normalized_X, y=y, cv = 10)\n",
        "print(np.mean(scores))\n",
        "print(np.min(scores))\n",
        "print(np.max(scores))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9334520597512481\n",
            "0.8431372549019608\n",
            "0.9630434782608696\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5zhXrPBgAUJw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "scores = cross_val_score(estimator=bdt,X=normalized_X, y=y, cv = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "9dc5cdea-6536-44c8-b77e-10ab1cd7c36d",
        "id": "JmT867UgAUJy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "print(np.mean(scores))\n",
        "print(np.min(scores))\n",
        "print(np.max(scores))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9151988601120749\n",
            "0.7932535364526659\n",
            "0.9521218715995647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lR_v1xfHAUJ1"
      },
      "cell_type": "markdown",
      "source": [
        "Results show that Logistic Regression and AdaBoost work best with this dataset after normalizing the data as capital length are skewed a little more then the other data dominating the answer if not min max normalized."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-uuq9bWJAUJ2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kPBUDPsZAtiG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using non linear support vector machine without normalization"
      ]
    },
    {
      "metadata": {
        "id": "-yegQKtSECIT",
        "colab_type": "code",
        "outputId": "144a98a2-ab79-4c68-a426-c49fb64d8a83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "import numpy as np\n",
        "\n",
        "model = svm.SVC(degree=3)\n",
        "model.fit(train_X,train_y)\n",
        "predict_y = model.predict(test_X)\n",
        "predict_y = [1 if y>=0.5 else 0 for y in predict_y]\n",
        "accuracy = np.sum(predict_y == test_y)/len(test_y)\n",
        "print(accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8218940052128584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VsCPsJo4BHzP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Using Normalized Data with SVM"
      ]
    },
    {
      "metadata": {
        "id": "0v2_CvZlBLdv",
        "colab_type": "code",
        "outputId": "5518fb15-097a-4b03-e17c-140d51b63294",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "normalized_X=(X-X.min())/(X.max()-X.min())\n",
        "n_train_X, n_test_X, n_train_y, n_test_y = train_test_split(normalized_X.as_matrix(), y.as_matrix(), test_size=0.25)\n",
        "\n",
        "model = svm.SVC(degree=3)\n",
        "model.fit(n_train_X,n_train_y)\n",
        "predict_y = model.predict(n_test_X)\n",
        "accuracy = np.sum(predict_y == n_test_y)/len(n_test_y)\n",
        "print(accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8071242397914856\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2SrukVQqKPdu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "scores = cross_val_score(estimator=model,X=normalized_X, y=y, cv = 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GJwasoslKYa9",
        "colab_type": "code",
        "outputId": "8c5dce5e-6078-474b-8e7b-c00a4b56991f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(np.max(scores))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8543478260869565\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n_gE3CHnKNas",
        "colab_type": "code",
        "outputId": "56da7116-2bb0-44f7-f0ea-545cfe92c2ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "np.random.seed(5)\n",
        "\n",
        "kmeans = KMeans(n_clusters = 2, random_state = 1).fit(train_X)\n",
        "\n",
        "predict_y = kmeans.predict(test_X)\n",
        "accuracy = np.sum(predict_y == (test_y))/len(test_y)\n",
        "print(accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6264118158123371\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mqoyHriYBx-X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Decision Tree without and with normalization"
      ]
    },
    {
      "metadata": {
        "id": "XWH030ZNIY-g",
        "colab_type": "code",
        "outputId": "1d37753a-e85d-4c4a-f93a-5ba42b535a99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "dtr = DecisionTreeRegressor(random_state=1, splitter='best')\n",
        "dtr.fit(train_X, train_y)\n",
        "predict_y = dtr.predict(test_X)\n",
        "accuracy = np.sum(predict_y == (test_y))/len(test_y)\n",
        "print(accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9252823631624674\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GSLW7d1ikp17",
        "colab_type": "code",
        "outputId": "9c618e6f-9a5f-4992-8b82-9a88fa1ff4c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "normalized_X=(X-X.min())/(X.max()-X.min())\n",
        "n_train_X, n_test_X, n_train_y, n_test_y = train_test_split(normalized_X.as_matrix(), y.as_matrix(), test_size=0.25)\n",
        "dtr.fit(n_train_X, n_train_y)\n",
        "predict_y = dtr.predict(n_test_X)\n",
        "accuracy = np.sum(predict_y == (n_test_y))/len(n_test_y)\n",
        "print(accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9261511728931364\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HKqN6jqJCO8I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Decision Tree Regressor with standardization (normal distribution of data)"
      ]
    },
    {
      "metadata": {
        "id": "OBOBvGAHlXTA",
        "colab_type": "code",
        "outputId": "96cb9854-f197-435c-f4b9-c1d6ff97ade9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "standardized_X=(X-X.mean())/X.std()\n",
        "n_train_X, n_test_X, n_train_y, n_test_y = train_test_split(normalized_X.as_matrix(), y.as_matrix(), test_size=0.25)\n",
        "dtr.fit(n_train_X, n_train_y)\n",
        "predict_y = dtr.predict(n_test_X)\n",
        "accuracy = np.sum(predict_y == (n_test_y))/len(n_test_y)\n",
        "print(accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.89748045178106\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p4JSbe1DC554",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Logistic Regression without and with normalization with cross validation"
      ]
    },
    {
      "metadata": {
        "id": "ELV-hwv3NQbi",
        "colab_type": "code",
        "outputId": "aa4675ed-13e4-4654-9074-c48d484a8985",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "clf = LogisticRegressionCV(cv=5, random_state=1,multi_class='ovr', solver='lbfgs').fit(X, y)\n",
        "print(clf.score(X,y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9311019343620952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eXNKuZfkUmIP",
        "colab_type": "code",
        "outputId": "2b577d15-c01b-4357-d1d6-706ed1617d7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "normalized_X=(X-X.min())/(X.max()-X.min())\n",
        "\n",
        "clf = LogisticRegressionCV(cv=5, random_state=1,multi_class='ovr', solver='lbfgs').fit(normalized_X, y)\n",
        "print(clf.score(normalized_X,y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9330580308628559\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_jOwlx_lDk8f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Logistic Regression seems to work the best along with cross validation and normalization.\n",
        "\n",
        "In the following one, I am trying it with some of the specific features and normalization on those features."
      ]
    },
    {
      "metadata": {
        "id": "OUitiPv_SxDn",
        "colab_type": "code",
        "outputId": "27329767-82cf-4bf5-8806-eb2a9da86162",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "df_features = ['capital_run_length_average',\n",
        "               'capital_run_length_longest',\n",
        "               'capital_run_length_total', \n",
        "               'word_freq_credit',\n",
        "               'char_freq_$',\n",
        "               'word_freq_meeting',\n",
        "               'char_freq_!',\n",
        "               'word_freq_conference',\n",
        "               'word_freq_table',\n",
        "                'word_freq_project',\n",
        "               'word_freq_free',\n",
        "               'word_freq_edu',\n",
        "               'word_freq_cs',\n",
        "               'char_freq_;',\n",
        "               'word_freq_direct',\n",
        "               'word_freq_receive',\n",
        "               'word_freq_money',\n",
        "               'word_freq_report',\n",
        "               'word_freq_lab',\n",
        "               'word_freq_pm',\n",
        "               'word_freq_order',\n",
        "               'word_freq_internet',\n",
        "               'word_freq_re',\n",
        "               'word_freq_our',\n",
        "               'char_freq_(',\n",
        "               'char_freq_[',\n",
        "               'word_freq_technology',\n",
        "               'word_freq_parts',\n",
        "               'word_freq_you',\n",
        "               'word_freq_your',\n",
        "               'word_freq_telnet']\n",
        "s_X = df[df_features]\n",
        "normalized_X=(s_X-s_X.min())/(s_X.max()-s_X.min())\n",
        "\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "clf = LogisticRegressionCV(cv=5, random_state=1,multi_class='ovr', solver='lbfgs').fit(normalized_X, y)\n",
        "print(clf.score(normalized_X,y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8969789176266029\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8eKLNMFjFUfm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "AdaBoost with Normalization and Cross Validation"
      ]
    },
    {
      "metadata": {
        "id": "B6Abps5We7q6",
        "colab_type": "code",
        "outputId": "743da0c6-6b4a-4f4c-d3ba-7b8c7aff1c6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "normalized_X=(X-X.mean())/X.max()-X.mean()\n",
        "\n",
        "bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n",
        "                         algorithm=\"SAMME.R\",\n",
        "                         n_estimators=200)\n",
        "\n",
        "\n",
        "\n",
        "scores = cross_val_score(estimator=bdt,X=normalized_X, y=y, cv = 10)\n",
        "print(np.mean(scores))\n",
        "print(np.min(scores))\n",
        "print(np.max(scores))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9334520597512481\n",
            "0.8431372549019608\n",
            "0.9630434782608696\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TuuprK8AJbCv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "scores = cross_val_score(estimator=bdt,X=normalized_X, y=y, cv = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fll5d8itJfH4",
        "colab_type": "code",
        "outputId": "9dc5cdea-6536-44c8-b77e-10ab1cd7c36d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "print(np.mean(scores))\n",
        "print(np.min(scores))\n",
        "print(np.max(scores))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9151988601120749\n",
            "0.7932535364526659\n",
            "0.9521218715995647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZcUx5uSxKi1S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Results show that Logistic Regression and AdaBoost work best with this dataset after normalizing the data as capital length are skewed a little more then the other data dominating the answer if not min max normalized."
      ]
    },
    {
      "metadata": {
        "id": "rC8il1wzJkeT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}